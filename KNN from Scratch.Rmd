---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
```{r}
#loading the necessary package
library(tidyverse)
library(lubridate)
library(stringr)
library(fastDummies)
library(caret)
library(Hmisc)
```


```{r}
#loading the dataset into the R environment
#glass <- read.csv("/Users/jess/Documents/Spring 2019/DA5030 - Intro to machine learning and data mining/Week 4- practicum 1/glassdata.csv", header = FALSE)
glass <- read.csv("/Users/hpj/Downloads/glass.data.csv", header = FALSE)
#naming the columns from the dataset
names(glass) <- c("id", "ri", "na", "mg", "al", "si", "k", "ca", "ba" ,"fe", "type")

```


```{r}
#changing the type of the variable "type" from the dataset
glass$type <- factor(glass$type)
```


```{r}
hist(glass$na)


# The kNN algorith is non-parametric because it takes in more training cases, and the training therefore grows and does not stay the same making it parametric. Non-parametric methods do not have a fixed number of parameters and neither does KNN.
```


```{r}
glass <- glass[,!names(glass) %in% c("id")]
head(glass)

normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
  if(max(x) - min(x) == 0) {
    return(0)
    
  }
  
}

glass_norm <- as.data.frame(lapply(glass[1:9], normalize))
head(glass_norm)
```

```{r}

set.seed(1)
new <- sample.int(n = nrow(glass_norm), size = floor(.50*nrow(glass_norm)), replace = F)
train_glass <- glass_norm[new,]
train_target_glass <- glass$type[new]

test_glass <- glass_norm[-new,]
test_target_glass <- glass$type[-new]

```


```{r}
knn_custom <- function(df, y, x, k) {
dist <- function(p , q)
{
  d <- 0
  for (i in 1:length(p)) {
    d <- d + (p[i] - q[i]) ^ 2
  }
  dist <- sqrt(d)
}

#creating a vector
neighbors <- function(glass_norm , unknown){
  m<- nrow(glass_norm)
  ds <- numeric(m)
  for (i in 1:m) {
    p<- glass_norm[i,]
    q <- as.numeric(unknown)
    ds[i] <- dist(p,q)
    
  }
  neighbors <- ds
}

#finding K-closest neighbor
k.closest <- function(neighbor, k)
{
  ordered.neighbors <- order(unlist(neighbor))
  k.closest <- ordered.neighbors[1:k]
}

#mode function for selecting the most common
Mode <- function(x){
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
#knn function

  nb <- neighbors(df, x)
  f <- k.closest(nb, k)
  knn_custom <- Mode(unlist(y[f]))
  return(knn_custom)
}
```

```{r}
#create the unknown cases as tibbles
unknown1 <- tibble::tibble(ri = 1.51721 ,na = 12.53, mg= 3.48, al = 1.39, si = 73.39, k =0.60, ca = 8.55, ba = 0.00,fe = 0.08)
unknown2 <- tibble::tibble(ri = 1.4897 ,na = 12.71, mg= 1.85, al = 1.81, si = 72.69, k =0.52, ca = 10.01, ba = 0.00,fe = 0.02)

#bind them together to normalize them to one dataframe
unknown <- rbind(unknown1, unknown2)
unknown <- as.data.frame(unknown)

#create a dataframe which has all the columns from actual dataset except "id" and "type"
before_norm_whole <- glass[,!names(glass) %in% c("type")]
head(before_norm_whole)

#joining the unknown and the dataframe which does not have "id" and "variable"
glass_beforenorm_whole<- rbind(unknown, before_norm_whole)
 head(glass_beforenorm_whole)
 
 #normalizing unknown and the actual data together
 glass_afternorm_whole <- as.data.frame(lapply(glass_beforenorm_whole[1:9], normalize))
 head(glass_afternorm_whole)
 
 #seperating the unknown and rest of the normalized dataset
 unknown <- glass_afternorm_whole[1:2,]
 glass_norm <- glass_afternorm_whole[-c(1,2), ]

 head(glass_norm)

 #seperate them again
unknown1 <- unknown[1, ]
unknown2 <- unknown[2, ]


y <- tibble::tibble(glass[, 10])
k <- 10

#get first case prediction
(a <- knn_custom(glass_norm,glass$type,unknown1,k))

#get second case prediction
(b <- knn_custom(glass_norm,glass$type,unknown2,k))
 


```


```{r}
#Applying knn from class package
# install.packages("class")
library(class)

(kn_pred_1 <- class::knn(train = glass_norm, test = unknown1, glass$type , k =14, use.all = F))


(kn_pred_2 <- class::knn(train = glass_norm, test = unknown2, glass$type , k =14, use.all = F))
```


```{r}
#creating a dataframe
knn_res <- data.frame(matrix(rep(NA, 14 * nrow(train_glass)), ncol = 14))
names(knn_res) <- 2:15

knn_res

#loop through

for (k in 2:15){
  
  knn_res[,as.character(k)] <- class::knn(train = train_glass, test=test_glass, cl=train_target_glass, k=k)
  
}
View(knn_res)

#finding errors 
error1 <- lapply(knn_res, actual = test_target_glass, function(column, actual){
  sum(column == actual)/length(column)
})

#since it producess logical list we have to unlist and store it as a dataframe
error1 <- data.frame(k = 2:15, e = unlist(error1))

#creating a ggplot.
ggplot(data = error1, mapping = aes(x = k, y = e))+
  geom_point()+
  geom_line()+
  labs(
    title = "K vs Error rate",
    xlab = "k",
    ylab = "Error Rate"
  )

```

```{r}
kn_pred <- class::knn(train = train_glass, test = test_glass, cl = train_target_glass, k =5)
library(gmodels)
CrossTable( x = test_target_glass, y = kn_pred, prop.chisq = FALSE)
```

```{r}
#house <- read.csv("/Users/jess/Documents/Spring 2019/DA5030 - Intro to machine learning and data mining/Week 4- practicum 1/kc_house_data.csv", header = TRUE)
house <- read.csv("/Users/hpj/Downloads/kc_house_data.csv", header = TRUE)
```


```{r}
#seperating "price" variable
target_data <- as.tibble(house[, 3])
names(target_data) <- c("price")


house <- house[,4:15]

house <- house[c("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "condition", "grade", "sqft_above", "sqft_basement", "yr_built", "waterfront", "view")]
#creating unknown dataset from question
train_data <- tibble(bedrooms=4,bathrooms=3,sqft_living=4852,sqft_lot=9812, floors=3,condition=3, grade=11, sqft_above = 1860, sqft_basement = 820, yr_built = 1962,waterfront=0,view =1 )

#joining the unknown and actual dataset to normalize it
house <- rbind(train_data, house)
```


```{r}
#normalizing the dataset
house_norm <- as.data.frame(lapply(house[,1:12], normalize))
#creating dummy variable for "view" variable
dum <- dummy_cols(house_norm, select_columns = "view", remove_first_dummy = TRUE)
#creating test data and unknown data
house_norm <- dum[-1, ]
train_data <- dum[1,]

head(house_norm)
house_norm <- as.data.frame(house_norm)
```

```{r}
#knn function


dist.reg <- function(p , q)
{
  d <- 0
  for (i in 1:length(p)) {
    d <- d + (p[i] - q[i]) ^ 2
  }
  dist.reg <- sqrt(d)
}

#creating a vector
neighbors.reg <- function(house_norm, train_data){
  m<- nrow(house_norm)
  ds <- numeric(m)
  for (i in 1:m) {
    p<- house_norm[i,]
    q <- as.numeric(train_data)
    ds[i] <- dist.reg(p,q)
    
  }
  neighbors.reg <- ds
  return(neighbors.reg)
}

#finding K-closest neighbor
k.closest <- function(neighbor.reg, k)
{
  ordered.neighbors.reg <- order(unlist(neighbor.reg))
  k.closest <- ordered.neighbors.reg[1:k]
  return(k.closest)
}



knn.reg <- function(new_data, target_data, train_data, k) {
  nb <- neighbors.reg(new_data, train_data)
  f <- k.closest(nb, k)
  avg <- as.numeric(unlist(target_data[f,]))
  knn.reg <- round(mean(avg))
  return(knn.reg)
}

```



```{r}
new_data <- house_norm[ , ]
new_data <- as.data.frame(new_data)
k <- 4
c <- knn.reg(new_data, target_data, train_data, k)
c
```


